{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "# Import datatypes for parameter type specification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Produce knn boundary graphs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Import datatypes for parameter type specification\n",
    "from pandas.core import frame\n",
    "from sklearn.preprocessing._label import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from a file to a dataframe. Drop any attributes with irrelevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "df = pd.read_csv(\"./ford.csv\")\n",
    "\n",
    "# drop unecessary columns\n",
    "df = df.drop([\"tax\"], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df.model.unique()\n",
    "fuelTypes = df.fuelType.unique()\n",
    "transmissions = df.transmission.unique()\n",
    "\n",
    "print(\"Models\\n\", models,\n",
    "      \"\\n\\nFuel Types\\n\", fuelTypes,\n",
    "      \"\\n\\nTransmissions\\n\", transmissions,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data to numeric format\n",
    "The target feature is car model. Use `sklearn.preprocessing.LabelEncoder()` to ordinalize the target feature. Also encode the other categorical features as 0 or 1 for each category using `pandas.get_dummies()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmissionNumeric = pd.get_dummies(df.transmission)\n",
    "df = pd.concat([df.drop(\"transmission\", axis=1), transmissionNumeric], axis=1)\n",
    "fuelTypeNumeric = pd.get_dummies(df.fuelType)\n",
    "df = pd.concat([df.drop(\"fuelType\", axis=1), fuelTypeNumeric], axis=1)\n",
    "display(df.head())\n",
    "df.to_csv(\"clean1_Ford.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop attributes or records to fine-tune the KNN model\n",
    "\n",
    "The primary dataset to be examined is stored to `df` and is exported to the \"clean1_Ford.csv\" file. The car models with high misclassification rates are stored in `df2` and exported to the \"clean2_Ford.csv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.model == ' Grand Tourneo Connect']\n",
    "df2 = pd.concat([df2, df[df.model == ' Tourneo Connect']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Transit Tourneo']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Tourneo Custom']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Fusion']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Ranger']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Streetka']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Escort']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Grand C-MAX']])\n",
    "df2 = pd.concat([df2, df[df.model == ' Mondeo']])\n",
    "df2 = pd.concat([df2, df[df.model == ' S-MAX']])\n",
    "df2.to_csv(\"clean2_Ford.csv\", index=False)\n",
    "\n",
    "df = df[df.model != ' Grand Tourneo Connect']\n",
    "df = df[df.model != ' Tourneo Connect']\n",
    "df = df[df.model != ' Transit Tourneo']\n",
    "df = df[df.model != ' Tourneo Custom']\n",
    "df = df[df.model != ' Fusion']\n",
    "df = df[df.model != ' Ranger']\n",
    "df = df[df.model != ' Streetka']\n",
    "df = df[df.model != ' Escort']\n",
    "df = df[df.model != ' Grand C-MAX']\n",
    "df = df[df.model != ' Mondeo']\n",
    "df = df[df.model != ' S-MAX']\n",
    "df.to_csv(\"clean1_Ford.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Scaling the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data for car models that have a low misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean1_Ford.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = df.drop(\"model\", axis=1)\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X.columns = X.columns + \" scaled\"\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "display(X_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_scaled = X_scaled[\"mpg scaled\"].to_frame()*20.0\n",
    "X_scaled = pd.concat([X_scaled.drop(\"mpg scaled\", axis=1), mpg_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.mean_).to_csv(\"mean.csv\", index=False)\n",
    "pd.DataFrame(scaler.var_).to_csv(\"variance.csv\", index=False)\n",
    "X_scaled.to_csv(\"X_scaled.csv\", index=False)\n",
    "df.model.to_csv(\"y.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process for the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"clean2_Ford.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "X2 = df.drop(\"model\", axis=1)\n",
    "scaler2.fit(X2)\n",
    "X2_scaled = scaler2.transform(X2)\n",
    "X2.columns = X2.columns + \" scaled\"\n",
    "X2_scaled = pd.DataFrame(X2_scaled, columns=X2.columns)\n",
    "display(X2_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_scaled = X2_scaled[\"mpg scaled\"].to_frame()*20.0\n",
    "X2_scaled = pd.concat([X2_scaled.drop(\"mpg scaled\", axis=1), mpg_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler2.mean_).to_csv(\"mean2.csv\", index=False)\n",
    "pd.DataFrame(scaler2.var_).to_csv(\"variance2.csv\", index=False)\n",
    "X2_scaled.to_csv(\"X2_scaled.csv\", index=False)\n",
    "df2.model.to_csv(\"y2.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Designing the KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scaled data for non-target attributes\n",
    "X_scaled = pd.read_csv(\"X_scaled.csv\")\n",
    "\n",
    "# read the target attribute\n",
    "y = pd.read_csv(\"y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a single column from a dataframe\n",
    "def encode_y(y:frame.DataFrame) -> dict:\n",
    "    y = y.squeeze()\n",
    "    labEnc_y = LabelEncoder()\n",
    "    labEnc_y = labEnc_y.fit(y)\n",
    "    y = pd.DataFrame(labEnc_y.transform(y), columns=[y.name])\n",
    "    return dict(y=y,encoder=labEnc_y)\n",
    "\n",
    "def unencode_y(y:frame.DataFrame, encoder:LabelEncoder):\n",
    "    # use sklearn.preprocessing.LabelEncoder.inverse_transform() \n",
    "    # to revert the encoded data\n",
    "    y = y.squeeze()\n",
    "    return(pd.DataFrame(encoder.inverse_transform(y), columns=[y.name]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### study interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_yItem = encode_y(y_train)\n",
    "labels_train = encode_yItem.get(\"y\")\n",
    "\n",
    "# # Use this variable to re-encode y labels\n",
    "# encoder = encode_yItem.get(\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(y_train.head())\n",
    "# pd.plotting.scatter_matrix(X_train_scaled,\n",
    "#                            c=labels_train.squeeze(),\n",
    "#                            hist_kwds={'bins':20},\n",
    "#                            figsize=(15,15))\n",
    "# plt.show()\n",
    "# print(type(y_train), type(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a 2d knn as an example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select two attributes for the example KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_2d = [\"mpg scaled\", \"price scaled\"]\n",
    "X2d_train = X_train_scaled.loc[:, names_2d]\n",
    "y_train = np.ravel(y_train)\n",
    "print(y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the sample model from the training set\n",
    "- show boundaries for the trained model.\n",
    "- points from the training set are labeled based on the car models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9, weights=\"distance\")\n",
    "knn.fit(X2d_train, y_train)\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X2d_train,\n",
    "    grid_resolution=100,\n",
    "    eps=0.1,\n",
    "    ax=ax,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    xlabel=names_2d[0],\n",
    "    ylabel=names_2d[1],\n",
    "    shading=\"auto\",\n",
    ")\n",
    "\n",
    "# Plot also the training points\n",
    "sns.scatterplot(\n",
    "    x=X2d_train[names_2d[0]],\n",
    "    y=X2d_train[names_2d[1]],\n",
    "    hue=y_train,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    "    s=3\n",
    ")\n",
    "\n",
    "# plt.gcf().set_size_inches(9,5)\n",
    "# plt.gcf().align_xlabels()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess accuracy for the sample KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2d_test = X_test_scaled.loc[:,[names_2d[0],names_2d[1]]]\n",
    "\n",
    "arr = np.array([])\n",
    "y_test = y_test.squeeze()\n",
    "for n in range(1,15):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X2d_train, y_train)\n",
    "    y_predict = knn.predict(X2d_test)\n",
    "    m = np.mean(y_predict == y_test)\n",
    "    arr = np.append(arr, np.array(m))\n",
    "plt.plot(range(1, arr.size+1), arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data still being used.\n",
    "# let's try a specific value of k\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X2d_train, y_train)\n",
    "y_predict = knn.predict(X2d_test)\n",
    "m = np.mean(y_predict == y_test)\n",
    "arr = np.append(arr, np.array(m))\n",
    "display(knn.score(X2d_test, y_test))\n",
    "\n",
    "misclassified = y_test[y_test != y_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = misclassified.unique()\n",
    "sum = 0\n",
    "error_counts = np.array([])\n",
    "name_counts = np.array([])\n",
    "for name in names:\n",
    "    errors = misclassified[misclassified == name].shape[0]\n",
    "    sum += errors\n",
    "    error_counts = np.append(error_counts, np.array(errors))\n",
    "    name_total = y_test[y_test == name].shape[0]\n",
    "    name_counts = np.append(name_counts, np.array(name_total))\n",
    "display(names, misclassified.shape, sum)\n",
    "\n",
    "error_percentage = 100 * error_counts / name_counts\n",
    "\n",
    "\n",
    "\n",
    "# Plot misclassifications\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_counts, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"Error counts per model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_percentage, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"Error percentage per model\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Decision Tree Comparision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree and a KNN from the full dataset and compare accuracies of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# import the scaled data and split into training and test sets\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(pd.read_csv(\"X_scaled.csv\"), pd.read_csv(\"y.csv\"), random_state=0)\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "display() # hacky way to clear the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tree.score(X_test_scaled, y_test), knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('env-infs768': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bd121de4aeb61ea8b61853af45da9c0e9dec6b603fbe91f85bea4ba32a7bbe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
