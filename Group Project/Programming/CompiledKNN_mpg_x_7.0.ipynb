{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision Tree for comparison\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.display import Image\n",
    "\n",
    "# If any figures change, set to true\n",
    "gen_new_plot=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1B: Initial comparison of KNN versus the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_Ford.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"model\", axis=1)\n",
    "y = df.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Drop attributes or records to fine-tune the KNN model\n",
    "\n",
    "A KNN model on the full dataset yields a high misclassification rate for certain cars such as the \"Tourneo Connect\" and the \"Fusion\" models. The cars with higher misclassification rates will isolated to a separate KNN model to study the behavior related to the high misclassification rates.\n",
    "\n",
    "The primary dataset to be examined is stored to `df` and is exported to the \"clean1_Ford.csv\" file. \n",
    "\n",
    "The car models with high misclassification rates are stored in `df2` and exported to the \"clean2_Ford.csv\" file. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Scaling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the dataset. Additionaly, multiply the \"mpg scaled\" attribute by a weight of `mpg_weight` to adjust the impact for groupings of this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_weight = 7.0\n",
    "\n",
    "# use mpg_col_name to rename the columns after adjusting the \"mpg scaled\" feature\n",
    "mpg_col_name = \"mpg scaled * \" + str(mpg_weight)\n",
    "print(\"mpg_col_name: \\'\" + mpg_col_name + \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled_mpg_x_7_0 = scaler.transform(X)\n",
    "columns = X.columns + \" scaled\"\n",
    "X_scaled_mpg_x_7_0 = pd.DataFrame(X_scaled_mpg_x_7_0, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the 'mpg scaled' feature\n",
    "mpg_scaled = X_scaled_mpg_x_7_0[\"mpg scaled\"].to_frame(name=mpg_col_name)*mpg_weight\n",
    "X_scaled_mpg_x_7_0 = pd.concat([X_scaled_mpg_x_7_0.drop(\"mpg scaled\", axis=1), mpg_scaled], axis=1)\n",
    "display(X_scaled_mpg_x_7_0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaled_mpg_x_7_0.to_csv(\"X_scaled_mpg_x_7.0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Designing the KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the scaled data from files. Store the features and targets to variables. These variables will be used for train-test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the target attribute\n",
    "y = y.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### study interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if (gen_new_plot):\n",
    "    # labels for pairwise plots\n",
    "    from sklearn.preprocessing._label import LabelEncoder\n",
    "\n",
    "    pd.plotting.scatter_matrix(\n",
    "        X_scaled_mpg_x_7_0[X_scaled_mpg_x_7_0.columns[[0,1,2,3,11]]],\n",
    "        c=LabelEncoder().fit(y).transform(y),\n",
    "        diagonal='hist',\n",
    "        hist_kwds={'bins':20},\n",
    "        figsize=(11, 8.5),\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    Image(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled_mpg_x_7_0, y,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train_scaled.shape, y_train.shape)\n",
    "display(X_test_scaled.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Decision Tree Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree and a KNN from the dataset and compare accuracies of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    random_state=0\n",
    ")\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# import the scaled data and split into training and test sets\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    pd.read_csv(\"X_scaled.csv\"), \n",
    "    pd.read_csv(\"y.csv\"), \n",
    "    random_state=0\n",
    ")\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# hacky way to clear the output\n",
    "display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracies on the test set\")\n",
    "print(\"tree accuracy: \" + str(tree.score(X_test_scaled, y_test)))\n",
    "print(\"knn accuracy: \" + str(knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree and a KNN from the secondary dataset and compare accuracies of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy of a 10-fold cross validation for the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.concat([X_scaled, X2_scaled], ignore_index=True, verify_integrity=True)\n",
    "features = X_scaled_mpg_x_7_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = pd.DataFrame(np.append(y, y2))\n",
    "target = pd.DataFrame(y, columns=[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = target.model.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.empty(shape=0, dtype=np.uint16)\n",
    "\n",
    "display(target.shape[0])\n",
    "\n",
    "for model in models:\n",
    "    mask = (target.model == model)\n",
    "    if (target[mask].shape[0] <= 50):\n",
    "        index = target[mask].index[:]\n",
    "        print(model, \"- remove\", index.shape[0], \"record(s)\")\n",
    "        indexes = np.append(indexes, index)\n",
    "\n",
    "target = target.drop(index=indexes, axis=0)\n",
    "features = features.drop(index=indexes, axis=0)\n",
    "\n",
    "display(target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = target.model.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "cv = cross_val_score(knn, features, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "cv_tree = cross_val_score(tree, features, target, cv=10)\n",
    "cv_tree.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Assessment on individual targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    features, target,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([])\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "\n",
    "for n in range(1,15):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y1_predict = knn.predict(X_test_scaled)\n",
    "    m = np.mean(y1_predict == y_test)\n",
    "    arr = np.append(arr, np.array(m))\n",
    "plt.plot(range(1, arr.size+1), arr)\n",
    "plt.title(\"K vs. Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_predict = knn.predict(X_test_scaled)\n",
    "display(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified_y = pd.DataFrame(y_test[y_test != y_predict], columns=[\"model\"])\n",
    "\n",
    "misclassified_y = y_test[y_test != y_predict]\n",
    "\n",
    "names = pd.Series(misclassified_y).unique()\n",
    "sum = 0\n",
    "error_counts = np.array([])\n",
    "name_counts = np.array([])\n",
    "for name in names:\n",
    "    errors = misclassified_y[misclassified_y == name].shape[0]\n",
    "    sum += errors\n",
    "    error_counts = np.append(error_counts, np.array(errors))\n",
    "    name_total = y_test[y_test == name].shape[0]\n",
    "    name_counts = np.append(name_counts, np.array(name_total))\n",
    "display(names, misclassified_y.shape, sum)\n",
    "\n",
    "error_percentage = 100 * error_counts / name_counts\n",
    "\n",
    "\n",
    "\n",
    "# Plot misclassifications\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_counts, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"KNN Final Model \\nError counts per model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_percentage, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"KNN Final Model \\nError percentage per model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Accuracy of decision tree for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    random_state=0\n",
    ")\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "y_predict = tree.predict(X_test_scaled)\n",
    "display(tree.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_y = y_test[y_test != y_predict]\n",
    "\n",
    "names = pd.Series(misclassified_y).unique()\n",
    "sum = 0\n",
    "error_counts = np.array([])\n",
    "name_counts = np.array([])\n",
    "for name in names:\n",
    "    errors = misclassified_y[misclassified_y == name].shape[0]\n",
    "    sum += errors\n",
    "    error_counts = np.append(error_counts, np.array(errors))\n",
    "    name_total = y_test[y_test == name].shape[0]\n",
    "    name_counts = np.append(name_counts, np.array(name_total))\n",
    "display(names, misclassified_y.shape, sum)\n",
    "\n",
    "error_percentage = 100 * error_counts / name_counts\n",
    "\n",
    "\n",
    "\n",
    "# Plot misclassifications\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_counts, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"Decision Tree \\nError counts per model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_percentage, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"Decision Tree \\nError percentage per model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy assessment without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features =  df.drop(\"model\", axis=1)\n",
    "target = df.model\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn2.fit(X_train, y_train)\n",
    "y_predict = knn2.predict(X_test)\n",
    "display(knn2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified_y = pd.DataFrame(y_test[y_test != y_predict], columns=[\"model\"])\n",
    "\n",
    "misclassified_y = y_test[y_test != y_predict]\n",
    "\n",
    "names = pd.Series(misclassified_y).unique()\n",
    "sum = 0\n",
    "error_counts = np.array([])\n",
    "name_counts = np.array([])\n",
    "for name in names:\n",
    "    errors = misclassified_y[misclassified_y == name].shape[0]\n",
    "    sum += errors\n",
    "    error_counts = np.append(error_counts, np.array(errors))\n",
    "    name_total = y_test[y_test == name].shape[0]\n",
    "    name_counts = np.append(name_counts, np.array(name_total))\n",
    "display(names, misclassified_y.shape, sum)\n",
    "\n",
    "error_percentage = 100 * error_counts / name_counts\n",
    "\n",
    "\n",
    "\n",
    "# Plot misclassifications\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_counts, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"KNN without scaling \\nError counts per model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(error_counts.size), height=error_percentage, align='edge', width=0.2)\n",
    "plt.xticks(range(error_counts.size), names, rotation=70)\n",
    "plt.title(\"KNN without scaling \\nError percentage per model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tree.feature_importances_)\n",
    "display(tree.feature_names_in_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bd121de4aeb61ea8b61853af45da9c0e9dec6b603fbe91f85bea4ba32a7bbe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
